"""
üß† VASI AI - GOD MODE ORCHESTRATOR üöÄ

Integrates the World's Fastest and Smartest open models:
- Groq (Llama-3-70B): Lightspeed intelligence (300+ tokens/s)
- Pollinations AI: Advanced Image Generation
- Document Generator: Professional PDF/Word creation
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from enum import Enum
from groq import Groq
from dotenv import load_dotenv
import requests
import base64

# Import document generator
try:
    from document_generator import doc_generator
    DOC_GEN_AVAILABLE = True
except ImportError:
    DOC_GEN_AVAILABLE = False
    print("[WARNING] Document generator not available")

# Import Nano Banana image generator
try:
    from .nano_banana import NanoBananaImageGenerator
    NANO_BANANA_AVAILABLE = True
except ImportError:
    NANO_BANANA_AVAILABLE = False
    print("[WARNING] Nano Banana image generator not available")

# Load environment variables
load_dotenv()

class ModelProvider(Enum):
    """Available AI model providers"""
    GROQ = "groq"
    HUGGINGFACE = "huggingface"

class SuperAdvancedOrchestrator:
    """
    GOD MODE Orchestrator
    """
    
    def __init__(self):
        self._load_keys()
        self.available_models = []
        self._initialize_models()
        print(f"[INIT] Vasi AI God Mode initialized with {len(self.available_models)} super-models")
    
    def _load_keys(self):
        self.groq_key = os.getenv("GROQ_API_KEY")
        self.hf_key = os.getenv("HUGGINGFACE_API_KEY")
        
    def _initialize_models(self):
        """Initialize connections to the super-models"""
        self.available_models = []
        
        # Initialize Groq (The Speed Demon)
        if self.groq_key:
            try:
                self.groq_client = Groq(api_key=self.groq_key)
        img_words = ['image', 'picture', 'photo', 'pic', 'img', 'iamge', 'imge', 'snapshot']
        action_words = ['generate', 'create', 'make', 'draw', 'want', 'need', 'get', 'give', 'show', 'visualize', 'imagine', 'render']
        direct_phrases = ['image of', 'picture of', 'photo of', 'draw me', 'show me']
        
        has_img_word = any(word in prompt_lower for word in img_words)
        has_action = any(word in prompt_lower for word in action_words)
        has_direct = any(phrase in prompt_lower for phrase in direct_phrases)
        
        if (has_img_word and has_action) or has_direct:
            return 'image'
        
        # Code Generation Detection
        code_keywords = ['code', 'function', 'class', 'script', 'program', 'algorithm', 'debug', 'fix', 'implement']
        if any(k in prompt_lower for k in code_keywords):
            return 'code'
        return 'general'

    async def generate_response(
        self, 
        prompt: str, 
        context: Optional[str] = None,
        use_reasoning: bool = True
    ) -> Dict[str, Any]:
        """
        Routing Logic:
        1. Document Generation Check
        2. Image Generation Check
        3. Groq (Llama-3) is Primary for Text/Code
        """
        task_type = self._detect_task_type(prompt)
        
        # Route Document Generation
        if task_type == 'document':
            return await self._generate_document(prompt)
        
        # Route Image Generation
        if task_type == 'image':
            return await self._generate_image(prompt)
        
        # Route to Groq for text/code
        enhanced_prompt = self._enhance_prompt(prompt, context, task_type)
        if ModelProvider.GROQ in self.available_models:
            return await self._call_groq(enhanced_prompt, task_type)
            
        return {
            "response": "Error: Groq API Key is missing or invalid. Please check .env", 
            "reasoning": [], 
            "model_used": "config-error",
            "confidence": 0.0
        }

    async def _generate_document(self, prompt: str) -> Dict[str, Any]:
        """Generate professional PDF or Word document"""
        if not DOC_GEN_AVAILABLE:
            return {
                "response": "‚ö†Ô∏è Document generation not available. Please install: pip install reportlab python-docx",
                "reasoning": ["Missing libraries"],
                "model_used": "error",
                "confidence": 0.0
            }
        
        try:
            print(f"[DOCUMENT] Generating document for: {prompt}")
            
            # First, generate content using Groq
            content_prompt = f"Write detailed, professional content for: {prompt}. Format with clear headings using # symbols. Make it comprehensive and well-structured."
            content_response = await self._call_groq(content_prompt, 'general')
            content = content_response['response']
            
            # Extract title from prompt
            title = prompt.replace('generate', '').replace('create', '').replace('pdf', '').replace('document', '').replace('word', '').strip()
            if not title:
                title = "Vasi AI Generated Document"
            
            # Determine format (PDF or Word)
            format_type = 'pdf' if 'pdf' in prompt.lower() else 'docx'
            
            # Generate document
            if format_type == 'pdf':
                filepath, doc_b64 = doc_generator.generate_pdf(title, content)
                if filepath:
                    download_link = f'<a href="data:application/pdf;base64,{doc_b64}" download="{os.path.basename(filepath)}" style="display:inline-block;margin:10px 0;padding:12px 24px;background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:white;text-decoration:none;border-radius:8px;font-weight:600;">üìÑ Download PDF</a>'
                    return {
                        "response": f"‚úÖ **PDF Document Generated Successfully!**\n\n**Title:** {title}\n\n{download_link}\n\n**Preview:**\n{content[:500]}...",
                        "reasoning": ["Generated content with Groq", "Created PDF with ReportLab"],
                        "model_used": "groq + pdf-generator",
                        "confidence": 1.0
                    }
            else:
                filepath, doc_b64 = doc_generator.generate_word(title, content)
                if filepath:
                    download_link = f'<a href="data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,{doc_b64}" download="{os.path.basename(filepath)}" style="display:inline-block;margin:10px 0;padding:12px 24px;background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:white;text-decoration:none;border-radius:8px;font-weight:600;">üìù Download Word Document</a>'
                    return {
                        "response": f"‚úÖ **Word Document Generated Successfully!**\n\n**Title:** {title}\n\n{download_link}\n\n**Preview:**\n{content[:500]}...",
                        "reasoning": ["Generated content with Groq", "Created DOCX with python-docx"],
                        "model_used": "groq + docx-generator",
                        "confidence": 1.0
                    }
            
            return {
                "response": "‚ö†Ô∏è Document generation failed. Please try again.",
                "reasoning": ["Generation error"],
                "model_used": "error",
                "confidence": 0.0
            }
            
        except Exception as e:
            print(f"[DOCUMENT] Error: {str(e)}")
            return {
                "response": f"‚ö†Ô∏è Document generation error: {str(e)}",
                "reasoning": ["Exception occurred"],
                "model_used": "error",
                "confidence": 0.0
            }

    async def _call_groq(self, prompt: str, task_type: str) -> Dict[str, Any]:
        """Call Groq API (Primary)"""
        try:
            # Use Llama 3.3 70B for max intelligence (Latest & Greatest)
            model_id = "llama-3.3-70b-versatile" 
            
            # Specialized System Prompts
            if task_type == 'code':
                sys_msg = "You are Vasi AI, an elite coding assistant. Write clean, efficient, production-ready code. Provide the code first, then brief explanations. Do not generate repetitive text."
            else:
                sys_msg = "You are Vasi AI, a hyper-intelligent assistant. Be specific and helpful. Avoid gibberish or repetition."

            completion = await asyncio.to_thread(
                self.groq_client.chat.completions.create,
                model=model_id,
                messages=[
                    {"role": "system", "content": sys_msg},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2048,
                top_p=0.9,
                frequency_penalty=0.8,
                presence_penalty=0.6,
                stop=["<|eot_id|>", "<|start_header_id|>", "<|end_of_text|>"],
                stream=False
            )
            
            text = completion.choices[0].message.content
            return {
                "response": text,
                "reasoning": ["Groq Instant Inference", "Llama-3-70B Reasoning"],
                "model_used": model_id,
                "confidence": 1.0
            }

        except Exception as e:
            return {"response": f"Groq Error: {str(e)}", "reasoning": [], "model_used": "error", "confidence": 0.0}

    async def _generate_image(self, prompt: str) -> Dict[str, Any]:
        """Generate image using Nano Banana (Gemini 2.5 Flash Image - FREE!)"""
        
        # Check if Nano Banana is available
        if not self.nano_banana:
            # Fallback to Pollinations if Nano Banana not available
            return await self._generate_image_pollinations(prompt)
        
        try:
            print(f"[NANO BANANA] Generating image: {prompt[:50]}...")
            
            # Clean the prompt (remove "image of", "picture of", etc.)
            clean_prompt = prompt.lower()
            for prefix in ['image of', 'picture of', 'photo of', 'generate', 'create', 'make', 'draw']:
                clean_prompt = clean_prompt.replace(prefix, '').strip()
            
            if not clean_prompt:
                clean_prompt = prompt
            
            # Generate image using Nano Banana
            result = await self.nano_banana.generate_image(
                prompt=clean_prompt,
                aspect_ratio="1:1",
                num_images=1
            )
            
            if result.get("success") and result.get("images"):
                image_url = result["images"][0]["url"]
                
                print(f"[NANO BANANA] ‚úÖ Image generated successfully!")
                
                # Return ONLY the image markdown
                return {
                    "response": f"![Generated Image]({image_url})",
                    "reasoning": ["Generated with Gemini 2.5 Flash Image (Nano Banana)"],
                    "model_used": "gemini-2.5-flash-image",
                    "confidence": 1.0
                }
            else:
                error_msg = result.get("error", "Unknown error")
                print(f"[NANO BANANA] ‚ùå Failed: {error_msg}")
                
                # Fallback to Pollinations
                print("[FALLBACK] Trying Pollinations AI...")
                return await self._generate_image_pollinations(prompt)
                
        except Exception as e:
            print(f"[NANO BANANA] ‚ùå Error: {str(e)}")
            
            # Fallback to Pollinations
            print("[FALLBACK] Trying Pollinations AI...")
            return await self._generate_image_pollinations(prompt)
    
    async def _generate_image_pollinations(self, prompt: str) -> Dict[str, Any]:
        """Fallback image generation using Pollinations AI"""
        try:
            import urllib.parse
            
            # Clean prompt
            clean_prompt = prompt.lower()
            for prefix in ['image of', 'picture of', 'photo of', 'generate', 'create']:
                clean_prompt = clean_prompt.replace(prefix, '').strip()
            
            clean_prompt = clean_prompt.encode('ascii', 'ignore').decode('ascii')
            if not clean_prompt.strip():
                clean_prompt = "a beautiful image"
            
            encoded_prompt = urllib.parse.quote(clean_prompt)
            image_url = f"https://image.pollinations.ai/prompt/{encoded_prompt}?width=1024&height=1024&nologo=true"
            
            print(f"[POLLINATIONS] Generating: {clean_prompt}")
            
            response = requests.get(image_url, timeout=90)
            
            if response.status_code == 200:
                image_b64 = base64.b64encode(response.content).decode('utf-8')
                data_uri = f"data:image/jpeg;base64,{image_b64}"
                
                return {
                    "response": f"![Generated Image]({data_uri})",
                    "reasoning": ["Generated with Pollinations AI (Fallback)"],
                    "model_used": "pollinations-ai",
                    "confidence": 1.0
                }
            else:
                return {
                    "response": f"‚ö†Ô∏è Image generation failed. Please try again!",
                    "reasoning": ["Both Nano Banana and Pollinations failed"],
                    "model_used": "error",
                    "confidence": 0.0
                }
                
        except Exception as e:
            return {
                "response": f"‚ö†Ô∏è Image generation error: {str(e)}",
                "reasoning": ["Exception occurred"],
                "model_used": "error",
                "confidence": 0.0
            }

    def _enhance_prompt(self, prompt: str, context: str, task_type: str) -> str:
        """Add context with smart truncation to prevent token overflow"""
        # Limit context to prevent Groq API errors
        MAX_CONTEXT_CHARS = 3000  # ~750 tokens
        MAX_PROMPT_CHARS = 1500   # ~375 tokens
        
        # Truncate context if too long (keep most recent)
        if context and len(context) > MAX_CONTEXT_CHARS:
            # Keep only the last part of context (most recent messages)
            context = "..." + context[-MAX_CONTEXT_CHARS:]
        
        # Truncate prompt if too long
        if len(prompt) > MAX_PROMPT_CHARS:
            prompt = prompt[:MAX_PROMPT_CHARS] + "..."
        
        full_prompt = ""
        if context:
            full_prompt += f"CONTEXT:\n{context}\n\n"
        full_prompt += f"{prompt}"
        
        return full_prompt

    def get_status(self):
        status = {
            "models": [m.value for m in self.available_models],
            "primary": "groq",
            "capabilities": ["text", "code", "image", "document"]
        }
        
        # Add Nano Banana status
        if self.nano_banana:
            status["image_generator"] = {
                "name": "Gemini 2.5 Flash Image (Nano Banana)",
                "status": "operational",
                "free": True,
                "daily_limit": 500
            }
        else:
            status["image_generator"] = {
                "name": "Pollinations AI (Fallback)",
                "status": "operational",
                "free": True
            }
        
        return status
